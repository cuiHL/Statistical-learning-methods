### 决策树

  决策树（decision tree）是一种基本的分类和回归方法。

#### 决策树与if-then规则

  可以将决策树看成是一个if-then规则的集合

#### 决策树与条件概率分布

  决策树所表示的概率分布由给定条件下的条件概率分布组成。

#### 决策树学习

  决策树学习的损失函数通常是正则化的极大似然函数。

#### 信息增益

​    特征A对训练数据集$D$的信息增益$g(D, A)$，定义为集合$D$的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D/A)$ 之差，即

$$
g(D,A) = H(D) - H(D/A)
$$

一般地，熵 $H(Y)$ 与 $H(Y/X)$ 之差称为互信息。

#### 信息增益比

$$
G_R = \frac{G(D, A)}{H(D)}
$$

#### 决策树的剪枝

  决策树的剪枝通过极小化整体的损失函数来实现。设树 $T$ 的叶节点个数为 $|T|$ ，$t$ 是 $T$ 的叶节点，该叶节点有 $N_t$ 个样本点，其中 $k$ 类的样本点有 $k$ 个， $H_t(T)$ 为叶节点 $t$ 上的经验熵， $ \alpha >=0 $ 为参数，则决策树的损失函数定义为
  
$$
C_\alpha(T) = \sum_{t=1}^{|T|}N_tH_t(T) + \alpha|T|
$$

其中，

$$
H_t(T)=-\sum_{k}\frac{N_{ik}}{N_t}log\frac{N_{ik}}{N_t}
$$

记

$$
C(T)=\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{ik}log\frac{N_{ik}}{N_t}
$$

这时有

$$
C_\alpha=C(T) + \alpha|T|
$$

$C(T)$ 表示模型对训练的预测误差， $|T|$ 表示模型的复杂度，参数 $\alpha \geq 0$ 控制两者之间的影响。 $\alpha=0$ 表示只考虑模型和样本的拟合程度，不考虑模型的复杂度。

  剪枝就是当 $\alpha$ 确定时，选择损失函数最小的模型，即损失函数最小的子树。当 $\alpha$ 确定时，子树越大，往往与训练数据拟合越好，模型复杂度越高；相反，子树越小，模型复杂度就越低。

#### CART

  ID3和C4.5只适用于分类情况，CART同样是由特征选择、树生成和剪枝组成，既可以用于分类，也可以用于回归。决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数（Gini index）最小化准则，进行特征选择，生成二叉树。

##### 回归树生成

  一个回归树对应着输入空间（特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为 $M$ 个单元 $R_1$ , $R_2$ ,..., $R_m$ ，并且在每个单元 $R_m$ 上有一个固定的输出值 $c_m$ ，于是回归树模型可以表示为

$$
f(x) = \sum_{m=1}^{M}c_mI(x \in R_m)
$$

  用平方误差 $\sum_{x \in R_m}(y_i - f(x_i))^2$ 表示回归树对于训练数据的预测误差。

  寻找方法对输入空间进行划分，选择第 $j$ 个变量 $x^{(j)}$ 和它取的值作为切分变量和切分点，并定义两个区域：

$$
R_1(j,s)=\{x\ | \ x^{(s)} \leq s \}
$$

$$
R_2(j,s)=\{x|x^{(s)} \geq s\}
$$

然后寻找最优的切分变量和最优切分点，具体地，求解

$$
\underset {j,s}{min}[ \underset {c_1}{min} (y-c_1)^2 +  \underset {c2}{min}(y-c2)^2 ]
$$

对于固定输入变量 $j$ 可以以找到最优切分点 $s$

$$
\hat c_1  =  ave(y_i|x_i \in  R_1(j,s))
$$

$$
\hat c_2  =  ave(y_i|x_i \in  R_2(j,s))
$$

##### 分类树的生成

  基尼系数：分类问题中，假设有 $K$ 个类，样本点属于第 $K$ 类的概率为 $p_k$ ，则概率分布的基尼指数定义为：

$$
Gini(p) =  \sum_{k=1}^{K} p_k(1-p_k) = 1 - \sum_{k=1}^{K}p_k^2
$$

  对于二分类问题，若样本点属于第1个类的概率是$p$，则概率分布的基尼指数为

$$
Gini(p)=2p(1-p)
$$

  对于给定的样本集合 $D$ ，其基尼指数为

$$
Gini(D) = 1-\sum_{k=1}^{K}(\frac{C_k}{D})^2
$$

  这里，$C_k$ 是 $D$ 中属于第 $k$ 类的样本子集， $K$ 是类的个数。

  如果样本集合 $D$ 根据特征 $A$ 是否取某一可能值 $a$ 被分割成 $D_1$ 和 $D_2$ 

两部分，即

$$
D_1 = \{(x, y) \in D |  A(x) = a\},  D2=D -D_1
$$

则在特征 $A$ 的条件下，集合 $D$ 的基尼指数定义为

$$
Gini(D,A) = \frac{|D_1|}{|D|}Gini(|D_1|) + \frac{|D_2|}{D}Gini(|D_2|)
$$

基尼指数 $Gini(D)$ 表示集合 $D$ 的不确定性，基尼指数 $Gini(D,A)$ 表示经 $A=a$ 分割后集合 $D$ 的不确定性，基尼指数值越大，样本集合的不确定性也就越大，这一点与熵类似。
